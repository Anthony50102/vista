{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2e6bd3c1-a593-49f9-94e6-e528c238ab9f",
   "metadata": {},
   "source": [
    "## Imports and Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b8d14478-25a3-4cd2-bd45-7f7ce4c2490b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1mUsing Device: cuda\n",
      "\u001b[1mShowing animations: False\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import h5py\n",
    "import sys\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.animation as anim\n",
    "from IPython.display import HTML, display\n",
    "from neuralop.models import FNO\n",
    "from neuralop import Trainer\n",
    "from neuralop.training import AdamW\n",
    "from neuralop.utils import count_model_params\n",
    "from neuralop import LpLoss, H1Loss\n",
    "from neuralop.data.datasets import load_darcy_flow_small\n",
    "import torch.nn as nn\n",
    "import random\n",
    "import os\n",
    "import datetime\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "show_animations = False\n",
    "data_truncation = .5\n",
    "# Implement this after trying the conv net and fno output learning\n",
    "output_learning = False\n",
    "\n",
    "plt.rcParams['animation.embed_limit'] = 500\n",
    "\n",
    "print(f\"\\033[1mUsing Device: {device}\")\n",
    "print(f\"\\033[1mShowing animations: {show_animations}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5d526d6-168c-46fb-803c-caaa8e3d9334",
   "metadata": {},
   "source": [
    "### Data Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8723a1eb-72ba-4065-8e3e-019e64768e6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the data and save as .np\n",
    "\n",
    "data_dir = \"/work/10407/anthony50102/frontera/data/hw2d_sim/t600_d256x256_raw/\"\n",
    "\n",
    "train_files = [\"hw2d_sim_step0.025_end1_pts512_c11_k015_N3_nu5e-8_20250315142044_11702_0.h5\",\n",
    "               \"hw2d_sim_step0.025_end1_pts512_c11_k015_N3_nu5e-8_20250315142045_4677_2.h5\"]\n",
    "test_files = [\"hw2d_sim_step0.025_end1_pts512_c11_k015_N3_nu5e-8_20250316215751_19984_3.h5\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c3f5d267-2dd0-43ee-8117-70aa81462367",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[3]\u001b[39m\u001b[32m, line 17\u001b[39m\n\u001b[32m     15\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m h5py.File(data_dir + file, \u001b[33m'\u001b[39m\u001b[33mr\u001b[39m\u001b[33m'\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m f:\n\u001b[32m     16\u001b[39m     end_index = \u001b[38;5;28mint\u001b[39m(f[\u001b[33m'\u001b[39m\u001b[33mdensity\u001b[39m\u001b[33m'\u001b[39m].shape[\u001b[32m0\u001b[39m] * data_truncation)\n\u001b[32m---> \u001b[39m\u001b[32m17\u001b[39m     density = \u001b[43mf\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mdensity\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m[\u001b[49m\u001b[43m:\u001b[49m\u001b[43mend_index\u001b[49m\u001b[43m]\u001b[49m\n\u001b[32m     18\u001b[39m     potential = f[\u001b[33m'\u001b[39m\u001b[33mphi\u001b[39m\u001b[33m'\u001b[39m][:end_index]\n\u001b[32m     19\u001b[39m     gamma_n = f[\u001b[33m'\u001b[39m\u001b[33mgamma_n\u001b[39m\u001b[33m'\u001b[39m][:end_index]\n",
      "\u001b[36mFile \u001b[39m\u001b[32mh5py/_objects.pyx:54\u001b[39m, in \u001b[36mh5py._objects.with_phil.wrapper\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mh5py/_objects.pyx:55\u001b[39m, in \u001b[36mh5py._objects.with_phil.wrapper\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/scratch/10407/anthony50102/junk_envs/venv_1765813401_2037/lib/python3.11/site-packages/h5py/_hl/dataset.py:840\u001b[39m, in \u001b[36mDataset.__getitem__\u001b[39m\u001b[34m(self, args, new_dtype)\u001b[39m\n\u001b[32m    838\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._fast_read_ok \u001b[38;5;129;01mand\u001b[39;00m (new_dtype \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[32m    839\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m840\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_fast_reader\u001b[49m\u001b[43m.\u001b[49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    841\u001b[39m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m:\n\u001b[32m    842\u001b[39m         \u001b[38;5;28;01mpass\u001b[39;00m  \u001b[38;5;66;03m# Fall back to Python read pathway below\u001b[39;00m\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "def process(density, potential, gamma_n, gamma_c):\n",
    "    data = np.concatenate(\n",
    "        (np.expand_dims(density, 1), np.expand_dims(potential, 1)),\n",
    "        axis=1)\n",
    "    derived_data = np.concatenate(\n",
    "        (np.expand_dims(gamma_n, 1), np.expand_dims(gamma_c, 1)),\n",
    "        axis=1)\n",
    "\n",
    "    return data, derived_data\n",
    "\n",
    "\n",
    "processed_train_files = []\n",
    "\n",
    "for file in train_files:\n",
    "    with h5py.File(data_dir + file, 'r') as f:\n",
    "        end_index = int(f['density'].shape[0] * data_truncation)\n",
    "        density = f['density'][:end_index]\n",
    "        potential = f['phi'][:end_index]\n",
    "        gamma_n = f['gamma_n'][:end_index]\n",
    "        gamma_c = f['gamma_c'][:end_index]\n",
    "        data, derived_data = process(density, potential, gamma_n, gamma_n)\n",
    "\n",
    "        save_name = \"train_\" + \"\".join(file.split(\".\")[:-1]) + \".npz\"\n",
    "        processed_train_files.append(save_name)\n",
    "\n",
    "        np.savez(\n",
    "            save_name,\n",
    "            data=data,\n",
    "            derived_data=derived_data\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7eef5b28-d867-4893-99ef-a24b1bf3783b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Animation of the training data\n",
    "if show_animations:\n",
    "    train_data = np.load(processed_train_files[0])[\"data\"][::25]\n",
    "\n",
    "    fig, ax = plt.subplots()\n",
    "\n",
    "    vmin = train_data[:, 0, ...].min()\n",
    "    vmax = train_data[:, 0, ...].max()\n",
    "\n",
    "    img = plt.imshow(train_data[0, 0, ...], vmin=vmin, vmax=vmax)\n",
    "\n",
    "    def animate(frame):\n",
    "        img.set_data(train_data[frame, 0, ...])\n",
    "        return [img]\n",
    "\n",
    "    plt.rcParams['animation.embed_limit'] = 500\n",
    "    animation = anim.FuncAnimation(fig, animate, frames=int(train_data.shape[0]), interval=20, blit=True)\n",
    "\n",
    "    display(HTML(animation.to_jshtml()))\n",
    "else:\n",
    "    print(\"Animation is turned off\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd7d7158-02dc-4b26-8540-d093384b41fa",
   "metadata": {},
   "source": [
    "### Dataset and Loaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b47baa21-4944-4c2e-9f01-600e2ef7e94a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "\n",
    "class ForwardPredictionDataset(Dataset):\n",
    "    \"\"\"\n",
    "    Dataset for learning forward prediction operators.\n",
    "\n",
    "    Args:\n",
    "        data_path: Path to the numpy array file\n",
    "        input_steps: Number of past timesteps to use as input\n",
    "        output_steps: Number of future timesteps to predict\n",
    "        stride: Stride between consecutive samples (default: 1)\n",
    "        train_split: Fraction of data to use for training (default: 0.8)\n",
    "        mode: 'train', 'val', or 'test'\n",
    "        val_split: Fraction of remaining data for validation (default: 0.5)\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, data_path, input_steps=1, output_steps=1, stride=1,\n",
    "                 train_split=0.8, val_split=0.5, mode='train'):\n",
    "        # Load data: (t, channel, x_dim, y_dim)\n",
    "        self.data = np.load(data_path)[\"data\"]\n",
    "        self.input_steps = input_steps\n",
    "        self.output_steps = output_steps\n",
    "        self.stride = stride\n",
    "\n",
    "        total_steps = self.data.shape[0]\n",
    "        sequence_length = input_steps + output_steps\n",
    "\n",
    "        # Split data temporally\n",
    "        train_end = int(total_steps * train_split)\n",
    "        val_end = train_end + int((total_steps - train_end) * val_split)\n",
    "\n",
    "        if mode == 'train':\n",
    "            self.data = self.data[:train_end]\n",
    "        elif mode == 'val':\n",
    "            self.data = self.data[train_end:val_end]\n",
    "        elif mode == 'test':\n",
    "            self.data = self.data[val_end:]\n",
    "        else:\n",
    "            raise ValueError(f\"mode must be 'train', 'val', or 'test', got {mode}\")\n",
    "\n",
    "        # Calculate number of valid sequences\n",
    "        self.num_sequences = (len(self.data) - sequence_length) // stride + 1\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.num_sequences\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # Calculate start index\n",
    "        start_idx = idx * self.stride\n",
    "\n",
    "        # Extract input and output sequences\n",
    "        input_seq = self.data[start_idx:start_idx + self.input_steps]\n",
    "        output_seq = self.data[start_idx + self.input_steps:\n",
    "                                start_idx + self.input_steps + self.output_steps]\n",
    "\n",
    "        # Convert to torch tensors\n",
    "        # TODO: Squeezing out the time dimension fix this\n",
    "        input_tensor = torch.from_numpy(input_seq).float().squeeze(0)  \n",
    "        output_tensor = torch.from_numpy(output_seq).float().squeeze(0)\n",
    "\n",
    "        return {'x': input_tensor,\n",
    "                'y': output_tensor,\n",
    "                't': start_idx,\n",
    "                'tend': start_idx + self.input_steps + self.output_steps - 1}\n",
    "\n",
    "\n",
    "class TrajDataset(Dataset):\n",
    "    # TODO: Look into LRU or memory mapping too speed this up\n",
    "    def __init__(self, data_path,\n",
    "                 train_split=0.8,\n",
    "                 val_split=0.5,\n",
    "                 mode='train'):\n",
    "        # Get all .npz files with full paths\n",
    "        self.data_path = data_path\n",
    "        all_files = [os.path.join(data_path, f) \n",
    "                     for f in os.listdir(data_path) \n",
    "                     if f.endswith(\".npz\")]\n",
    "\n",
    "        #TODO: Remove this filtering out of derived files\n",
    "        all_files = [file for file in all_files if 'derived' not in file]\n",
    "\n",
    "        self.num_files = len(all_files)\n",
    "\n",
    "        # Split files\n",
    "        train_end = int(self.num_files * train_split)\n",
    "        val_end = train_end + int((self.num_files - train_end) * val_split)\n",
    "\n",
    "        self.train_files = all_files[:train_end]\n",
    "        self.val_files = all_files[train_end:val_end]\n",
    "        self.test_files = all_files[val_end:]\n",
    "\n",
    "        if mode == 'train':\n",
    "            self.avail_files = self.train_files\n",
    "        elif mode == 'val':\n",
    "            self.avail_files = self.val_files\n",
    "        elif mode == 'test':\n",
    "            self.avail_files = self.test_files\n",
    "        else:\n",
    "            raise ValueError(f\"mode must be 'train', 'val', or 'test', got {mode}\")\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.avail_files)  # Fixed: should be length of available files, not total\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        data = np.load(self.avail_files[idx])[\"data\"]\n",
    "        return {'data': data}\n",
    "\n",
    "\n",
    "# Example usage\n",
    "def create_dataloaders(data_path, input_steps=4, output_steps=1, \n",
    "                       batch_size=32, num_workers=4):\n",
    "    \"\"\"\n",
    "    Create train, validation, and test dataloaders.\n",
    "    \"\"\"\n",
    "    train_dataset = ForwardPredictionDataset(\n",
    "        data_path, input_steps=input_steps, output_steps=output_steps, mode='train'\n",
    "    )\n",
    "    val_dataset = ForwardPredictionDataset(\n",
    "        data_path, input_steps=input_steps, output_steps=output_steps, mode='val'\n",
    "    )\n",
    "    test_dataset = ForwardPredictionDataset(\n",
    "        data_path, input_steps=input_steps, output_steps=output_steps, mode='test'\n",
    "    )\n",
    "\n",
    "    train_loader = DataLoader(train_dataset, batch_size=batch_size,\n",
    "                              shuffle=True, num_workers=num_workers)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=batch_size,\n",
    "                            shuffle=False, num_workers=num_workers)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=batch_size,\n",
    "                             shuffle=False, num_workers=num_workers)\n",
    "\n",
    "    return train_loader, val_loader, test_loader\n",
    "\n",
    "\n",
    "def create_traj_loader(data_path, batch_size=1, num_workers=1):\n",
    "    dataset = TrajDataset(data_path)\n",
    "\n",
    "    loader = DataLoader(dataset, batch_size=batch_size,\n",
    "                        shuffle=True, num_workers=num_workers)\n",
    "    return loader\n",
    "\n",
    "\n",
    "# Usage example:\n",
    "data_path = processed_train_files[0]\n",
    "\n",
    "train_loader, val_loader, test_loader = create_dataloaders(\n",
    "    data_path,\n",
    "    input_steps=1,    # Use 4 past timesteps\n",
    "    output_steps=1,   # Predict 1 future timestep\n",
    "    batch_size=32\n",
    ")\n",
    "\n",
    "traj_loader = create_traj_loader(\".\")\n",
    "\n",
    "# Test the loader\n",
    "for sample in train_loader:\n",
    "    print(f\"Input shape: {sample['x'].shape}\")   # (batch, input_steps, channels, x_dim, y_dim)\n",
    "    print(f\"Target shape: {sample['y'].shape}\")  # (batch, output_steps, channels, x_dim, y_dim)\n",
    "    break\n",
    "\n",
    "for traj in traj_loader:\n",
    "    print(traj['data'].shape)\n",
    "    break\n",
    "print(len(traj_loader))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "857f4108-1266-4894-9db5-7cc3950b1b57",
   "metadata": {},
   "source": [
    "### Define Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4750d74-d0aa-411d-a09b-e9dd9e7a3ea0",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = FNO(\n",
    "    n_modes=(64, 64),\n",
    "    in_channels=2,\n",
    "    out_channels=2,\n",
    "    hidden_channels=512,\n",
    "    # projection_channel_ratio=2,\n",
    ")\n",
    "model = model.to(device)\n",
    "\n",
    "# Count and display the number of parameters\n",
    "n_params = count_model_params(model)\n",
    "print(f\"\\nOur model has {n_params} parameters.\")\n",
    "sys.stdout.flush()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ef2beec-b831-40ca-b8f9-60605ff9a5c7",
   "metadata": {},
   "source": [
    "### Define optim, scheduler, loss funcs, etc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d153fe31-5578-45ef-90e6-960c272b1efb",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = AdamW(model.parameters(), lr=1e-2, weight_decay=1e-4)\n",
    "scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=30)\n",
    "\n",
    "l2loss = LpLoss(d=2, p=2)  # L2 loss for function values\n",
    "h1loss = H1Loss(d=2)  # H1 loss includes gradient information\n",
    "\n",
    "train_loss = h1loss\n",
    "eval_losses = {\"h1\": h1loss, \"l2\": l2loss}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57d8d59d-d0aa-4767-b118-433e49a5ae4d",
   "metadata": {},
   "source": [
    "### Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c52a4ae0-f168-454c-8a05-47309d01eb3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import random\n",
    "from pathlib import Path\n",
    "\n",
    "# Hyperparameters\n",
    "max_epochs = 1000\n",
    "max_rollout = 90\n",
    "learning_rate = 1e-4  # Start lower based on your initial loss\n",
    "clip_norm = 10.0  # Increased - be less aggressive\n",
    "\n",
    "# Checkpoint\n",
    "checkpoint_dir = Path(\"/scratch/10407/anthony50102/checkpoints\")\n",
    "checkpoint_dir.mkdir(exist_ok=True)\n",
    "\n",
    "# State\n",
    "current_rollout = 1\n",
    "best_loss = float('inf')\n",
    "\n",
    "print(\"Starting training...\")\n",
    "\n",
    "for epoch in range(max_epochs):\n",
    "    epoch_loss = 0.0\n",
    "    n_steps = 0\n",
    "    \n",
    "    for batch in traj_loader:\n",
    "        traj = batch['data'].to(device).float()\n",
    "        batch_size, traj_len, c, h, w = traj.shape\n",
    "        \n",
    "        # Sample random starting point\n",
    "        max_start = traj_len - current_rollout - 1\n",
    "        if max_start < 0:\n",
    "            continue\n",
    "        start = random.randint(0, max_start)\n",
    "        \n",
    "        # Rollout with teacher forcing\n",
    "        state = traj[:, start].clone()\n",
    "        total_loss = 0.0\n",
    "        \n",
    "        for step in range(current_rollout):\n",
    "            pred = model(state)\n",
    "            target = traj[:, start + step + 1]\n",
    "            loss = nn.functional.mse_loss(pred, target)\n",
    "            total_loss += loss\n",
    "            \n",
    "            # Teacher forcing: gradually decay\n",
    "            teacher_ratio = max(0.3, 1.0 - epoch / 500)\n",
    "            if random.random() < teacher_ratio:\n",
    "                state = target.detach()\n",
    "            else:\n",
    "                state = pred.detach()\n",
    "        \n",
    "        # Backprop\n",
    "        optimizer.zero_grad()\n",
    "        total_loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), clip_norm)\n",
    "        optimizer.step()\n",
    "        \n",
    "        epoch_loss += total_loss.item()\n",
    "        n_steps += 1\n",
    "    \n",
    "    if n_steps == 0:\n",
    "        continue\n",
    "    \n",
    "    # Average loss\n",
    "    avg_loss = epoch_loss / n_steps\n",
    "    \n",
    "    # Save best checkpoint\n",
    "    if avg_loss < best_loss:\n",
    "        best_loss = avg_loss\n",
    "        torch.save({\n",
    "            'model': model.state_dict(),\n",
    "            'optimizer': optimizer.state_dict(),\n",
    "            'rollout': current_rollout,\n",
    "            'epoch': epoch,\n",
    "            'loss': best_loss\n",
    "        }, checkpoint_dir / \"best.pt\")\n",
    "    \n",
    "    # Gradually increase rollout\n",
    "    if epoch > 0 and epoch % 50 == 0 and current_rollout < max_rollout:\n",
    "        current_rollout = min(current_rollout + 5, max_rollout)\n",
    "        print(f\"→ Rollout increased to {current_rollout}\")\n",
    "    \n",
    "    # Status\n",
    "    if epoch % 10 == 0:\n",
    "        print(f\"Epoch {epoch+1:4d} | Loss: {avg_loss:.2f} | Rollout: {current_rollout:2d} | Best: {best_loss:.2f}\")\n",
    "\n",
    "print(f\"\\n✅ Done! Best loss: {best_loss:.2f}, Final rollout: {current_rollout}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df2cec59-dac8-469f-a726-e55025f5a645",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save model\n",
    "now = datetime.datetime.now()\n",
    "formatted_now = now.strftime(\"%m_%d_%Y-%H:%M\")\n",
    "torch.save(model.state_dict(), f'state_model_{formatted_now}.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5827b4e2-b04c-412e-a948-33d7eb701d00",
   "metadata": {},
   "outputs": [],
   "source": [
    "# trainer = Trainer(\n",
    "#     model=model,\n",
    "#     n_epochs=15,\n",
    "#     device=device,\n",
    "#     wandb_log=False,  # Disable Weights & Biases logging for this tutorial\n",
    "#     eval_interval=5,  # Evaluate every 5 epochs\n",
    "#     use_distributed=False,  # Single GPU/CPU training\n",
    "#     verbose=True,  # Print training progress\n",
    "# )\n",
    "\n",
    "# train_loader, test_loaders, data_processor = load_darcy_flow_small(\n",
    "#     n_train=1000,\n",
    "#     batch_size=64,\n",
    "#     n_tests=[100, 50],\n",
    "#     test_resolutions=[16, 32],\n",
    "#     test_batch_sizes=[32, 32],\n",
    "# )\n",
    "\n",
    "# trainer.train(\n",
    "#     train_loader=train_loader,\n",
    "#     test_loaders={256:test_loader},\n",
    "#     optimizer=optimizer,\n",
    "#     scheduler=scheduler,\n",
    "#     regularizer=False,\n",
    "#     training_loss=train_loss,\n",
    "#     eval_losses=eval_losses,\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e650d0d-7be0-4943-8fac-97ca1292d8e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def rollout(model, data, device=\"cuda\"):\n",
    "    \"\"\"\n",
    "    Autoregressive rollout:\n",
    "    - data: numpy array of shape (T, C, X, Y)\n",
    "    - model: forward prediction operator\n",
    "    - returns: reconstruction of full trajectory\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "\n",
    "    T, C, X, Y = data.shape\n",
    "\n",
    "    # Storage for reconstruction\n",
    "    recon = np.zeros_like(data)\n",
    "\n",
    "    # Initial condition (t=0)\n",
    "    current = torch.from_numpy(data[0]).float().to(device)\n",
    "    recon[0] = data[0]\n",
    "\n",
    "    for t in range(1, T):\n",
    "        # Model expects batch dimension\n",
    "        inp = current.unsqueeze(0)\n",
    "        with torch.no_grad():\n",
    "            pred = model(inp)  # output shape: (1, C, X, Y)\n",
    "        pred_np = pred.squeeze(0).cpu().numpy()\n",
    "        recon[t] = pred_np\n",
    "        # Feed output back in\n",
    "        current = pred.squeeze(0)\n",
    "\n",
    "    return recon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26e30a6c-1bd2-467a-9ad5-d69d92438393",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load full dataset (not split)\n",
    "full_data = np.load(data_path)[\"data\"]\n",
    "print(f\"Full data shape: {full_data.shape}\")\n",
    "\n",
    "# Determine temporal splits you used\n",
    "total_steps = full_data.shape[0]\n",
    "train_end = int(total_steps * 0.8)\n",
    "val_end = train_end + int((total_steps - train_end) * 0.5)\n",
    "\n",
    "train_data = full_data[:train_end]\n",
    "test_data  = full_data[val_end:]\n",
    "\n",
    "model = model.to(device)\n",
    "\n",
    "# Run rollouts\n",
    "train_recon = rollout(model, train_data, device=device)\n",
    "test_recon  = rollout(model, test_data, device=device)\n",
    "\n",
    "print(\"Train recon shape:\", train_recon.shape)\n",
    "print(\"Test  recon shape:\", test_recon.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "616739e5-7e10-47ea-956c-72a1c442d503",
   "metadata": {},
   "outputs": [],
   "source": [
    "if show_animations:\n",
    "    print(\"\\033[1mShowing animation\")\n",
    "    # Subsample\n",
    "    train_data_subbed = train_data[::5]\n",
    "    train_recon_subbed = train_recon[::5]\n",
    "\n",
    "    # Figure + axes\n",
    "    fig, ax = plt.subplots(1, 2, figsize=(8, 4))\n",
    "\n",
    "    t_vmin = train_data_subbed[:, 0].min()\n",
    "    t_vmax = train_data_subbed[:, 0].max()\n",
    "    # r_vmin = train_recon_subbed[:, 0].min()\n",
    "    # r_vmax = train_recon_subbed[:, 0].max()\n",
    "\n",
    "    # Create the images on the correct axes\n",
    "    img = ax[0].imshow(train_data_subbed[0, 0], vmin=t_vmin, vmax=t_vmax)\n",
    "    img2 = ax[1].imshow(train_recon_subbed[0, 0])\n",
    "\n",
    "    ax[0].set_title(\"Ground Truth\")\n",
    "    ax[1].set_title(\"Reconstruction\")\n",
    "\n",
    "\n",
    "    # Animation function\n",
    "    def animate(frame):\n",
    "        img.set_data(train_data_subbed[frame, 0])\n",
    "        img2.set_data(train_recon_subbed[frame, 0])\n",
    "        return [img, img2]\n",
    "\n",
    "\n",
    "    animation = anim.FuncAnimation(\n",
    "        fig,\n",
    "        animate,\n",
    "        frames=train_data_subbed.shape[0],\n",
    "        interval=20,\n",
    "        blit=True\n",
    "    )\n",
    "\n",
    "    display(HTML(animation.to_jshtml()))\n",
    "else:\n",
    "    print(\"Not showing animations\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c942763d-e2c8-406c-9015-b651ccc4e92e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (myenv)",
   "language": "python",
   "name": "myenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
