{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "97f74dbb-3e49-4372-913d-5214d716a02a",
   "metadata": {},
   "source": [
    "## FNO Output Learning: State → Derived Quantities\n",
    "\n",
    "### Architecture Overview\n",
    "This notebook trains a model to map **state(t) → derived(t)**, where:\n",
    "- **State**: (density, potential) - shape (2, H, W)\n",
    "- **Derived**: (gamma_n, gamma_c) - scalar values broadcast to (2, H, W) for compatibility\n",
    "\n",
    "### Combined Inference Pipeline\n",
    "1. **State Model** (from `fno_test.ipynb`): state(t) → state(t+1) (autoregressive)\n",
    "2. **Output Model** (this notebook): state(t) → derived(t) (direct mapping)\n",
    "\n",
    "Given only an initial condition state(0), we can predict the full trajectory AND derived quantities!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fdfd276-3b3b-4ccc-a2a6-d28c41b90ceb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1mUsing Device: cuda\n",
      "\u001b[1mShowing animations: True\n"
     ]
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "import torch\n",
    "import h5py\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.animation as anim\n",
    "from IPython.display import HTML, display\n",
    "from neuralop.models import FNO\n",
    "from neuralop.training import AdamW\n",
    "from neuralop.utils import count_model_params\n",
    "import torch.nn as nn\n",
    "import os\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "# Local utilities\n",
    "from utils import (\n",
    "    StateDerivedDataset, TrajectoryWithDerivedDataset,\n",
    "    train_epoch_direct, validate_direct,\n",
    "    rollout_combined, compute_mse_over_time, compute_mae_over_time, print_summary,\n",
    "    save_model, load_model\n",
    ")\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "show_animations = True\n",
    "data_truncation = 0.5\n",
    "\n",
    "plt.rcParams['animation.embed_limit'] = 500\n",
    "\n",
    "print(f\"\\033[1mUsing Device: {device}\")\n",
    "print(f\"\\033[1mShowing animations: {show_animations}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "649b5d4b-a071-42d6-9bdb-ee0b57742f40",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data paths\n",
    "data_dir = \"/work/10407/anthony50102/frontera/data/hw2d_sim/t600_d256x256_raw/\"\n",
    "\n",
    "train_files = [\"hw2d_sim_step0.025_end1_pts512_c11_k015_N3_nu5e-8_20250315142044_11702_0.h5\",\n",
    "               \"hw2d_sim_step0.025_end1_pts512_c11_k015_N3_nu5e-8_20250315142045_4677_2.h5\"]\n",
    "test_files = [\"hw2d_sim_step0.025_end1_pts512_c11_k015_N3_nu5e-8_20250316215751_19984_3.h5\"]\n",
    "\n",
    "# Path to pre-trained state model (update this after training fno_test.ipynb)\n",
    "STATE_MODEL_PATH = \"state_model_12_16_2025-12:00.pth\"  # Update with your actual path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01ab03ad-4b6c-4f25-ade2-cf8e8adc33af",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_with_derived(density, potential, gamma_n, gamma_c):\n",
    "    \"\"\"\n",
    "    Process data to include both state and derived quantities.\n",
    "    \n",
    "    Returns:\n",
    "        state: (T, 2, H, W) - density and potential\n",
    "        derived: (T, 2) - gamma_n and gamma_c scalars\n",
    "    \"\"\"\n",
    "    state = np.stack([density, potential], axis=1)  # (T, 2, H, W)\n",
    "    derived = np.stack([gamma_n, gamma_c], axis=1)  # (T, 2) - scalar values\n",
    "    return state, derived\n",
    "\n",
    "\n",
    "processed_files = []\n",
    "\n",
    "for file in train_files:\n",
    "    save_name = \"output_train_\" + \"\".join(file.split(\".\")[:-1]) + \".npz\"\n",
    "    \n",
    "    if os.path.exists(save_name):\n",
    "        print(f\"File already processed: {save_name}\")\n",
    "        processed_files.append(save_name)\n",
    "        continue\n",
    "    \n",
    "    with h5py.File(data_dir + file, 'r') as f:\n",
    "        end_index = int(f['density'].shape[0] * data_truncation)\n",
    "        density = f['density'][:end_index]\n",
    "        potential = f['phi'][:end_index]\n",
    "        gamma_n = f['gamma_n'][:end_index]\n",
    "        gamma_c = f['gamma_c'][:end_index]\n",
    "        \n",
    "        state, derived = process_with_derived(density, potential, gamma_n, gamma_c)\n",
    "        \n",
    "        np.savez(save_name, state=state, derived=derived)\n",
    "        processed_files.append(save_name)\n",
    "        print(f\"Saved: {save_name}\")\n",
    "\n",
    "print(f\"\\nProcessed {len(processed_files)} files\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "945f6af2-7ba7-4018-9e1f-a345f7f0ff2f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 8000, 4, 256, 256])\n"
     ]
    }
   ],
   "source": [
    "# Create dataloaders using shared Dataset classes\n",
    "data_path = processed_files[0]\n",
    "\n",
    "train_loader = DataLoader(StateDerivedDataset(data_path, mode='train'), batch_size=64, shuffle=True, num_workers=4)\n",
    "val_loader = DataLoader(StateDerivedDataset(data_path, mode='val'), batch_size=64, shuffle=False, num_workers=4)\n",
    "test_loader = DataLoader(StateDerivedDataset(data_path, mode='test'), batch_size=64, shuffle=False, num_workers=4)\n",
    "\n",
    "# Trajectory loader for rollout training\n",
    "traj_loader = DataLoader(TrajectoryWithDerivedDataset(\".\", mode='train'), batch_size=1, shuffle=True)\n",
    "\n",
    "# Test\n",
    "for batch in train_loader:\n",
    "    print(f\"State shape: {batch['state'].shape}\")\n",
    "    print(f\"Derived shape: {batch['derived'].shape}\")\n",
    "    break\n",
    "\n",
    "print(f\"\\nTrain: {len(train_loader.dataset)}, Val: {len(val_loader.dataset)}, Test: {len(test_loader.dataset)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f4eb7b3-5f54-450a-9c20-4df2eef5e2af",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============== Load Pre-trained State Model ==============\n",
    "state_model = FNO(n_modes=(64, 64), in_channels=2, out_channels=2, hidden_channels=512)\n",
    "state_model = state_model.to(device)\n",
    "\n",
    "# Uncomment to load trained weights:\n",
    "# load_model(state_model, 'state_model')\n",
    "# state_model.eval()\n",
    "# for p in state_model.parameters():\n",
    "#     p.requires_grad = False\n",
    "\n",
    "print(f\"State model: {count_model_params(state_model)} parameters\")\n",
    "print(\"NOTE: Uncomment load_model after training fno_test.ipynb!\")\n",
    "\n",
    "# ============== Output Model (FNO-based) ==============\n",
    "output_model = FNO(n_modes=(32, 32), in_channels=2, out_channels=2, hidden_channels=128, n_layers=3)\n",
    "output_model = output_model.to(device)\n",
    "\n",
    "print(f\"Output model: {count_model_params(output_model)} parameters\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce50e449-11da-4ff1-a498-a3277278176a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Since derived quantities are scalars, we need a pooling layer to map from spatial output\n",
    "class OutputHead(nn.Module):\n",
    "    \"\"\"Maps FNO output (B, 2, H, W) → scalar predictions (B, 2)\"\"\"\n",
    "    def __init__(self, hidden_dim=64):\n",
    "        super().__init__()\n",
    "        self.pool = nn.AdaptiveAvgPool2d(1)  # Global average pooling\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(2, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, 2)\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # x: (B, 2, H, W)\n",
    "        x = self.pool(x)  # (B, 2, 1, 1)\n",
    "        x = x.view(x.size(0), -1)  # (B, 2)\n",
    "        return self.fc(x)  # (B, 2)\n",
    "\n",
    "\n",
    "# Combined model: FNO features + pooling head\n",
    "class DerivedQuantityModel(nn.Module):\n",
    "    def __init__(self, fno_model, output_head):\n",
    "        super().__init__()\n",
    "        self.fno = fno_model\n",
    "        self.head = output_head\n",
    "    \n",
    "    def forward(self, x):\n",
    "        features = self.fno(x)  # (B, 2, H, W)\n",
    "        return self.head(features)  # (B, 2)\n",
    "\n",
    "\n",
    "output_head = OutputHead(hidden_dim=64).to(device)\n",
    "derived_model = DerivedQuantityModel(output_model, output_head).to(device)\n",
    "\n",
    "print(f\"Total derived model parameters: {sum(p.numel() for p in derived_model.parameters())}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ebc2dfb-9096-4ca6-a3de-e73c9e5a4253",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============== Training ==============\n",
    "print(\"=\" * 50)\n",
    "print(\"Training: Direct state → derived mapping (FNO)\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "optimizer = AdamW(derived_model.parameters(), lr=1e-3, weight_decay=1e-4)\n",
    "best_val_loss = float('inf')\n",
    "\n",
    "for epoch in range(100):\n",
    "    train_loss = train_epoch_direct(derived_model, train_loader, optimizer, device)\n",
    "    val_loss = validate_direct(derived_model, val_loader, device)\n",
    "    \n",
    "    if val_loss < best_val_loss:\n",
    "        best_val_loss = val_loss\n",
    "        save_model(derived_model, 'derived_fno', metadata={'best_val_loss': best_val_loss})\n",
    "    \n",
    "    if (epoch + 1) % 10 == 0:\n",
    "        print(f\"Epoch {epoch+1}/100 | Train: {train_loss:.6f} | Val: {val_loss:.6f}\")\n",
    "\n",
    "# Load best model\n",
    "load_model(derived_model, 'derived_fno')\n",
    "print(f\"\\nBest validation loss: {best_val_loss:.6f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "717735c6-6e10-4519-b81e-5d215dde3a5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model already saved during training with best_val_loss\n",
    "print(\"Best model saved as: derived_fno_latest.pt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20040252",
   "metadata": {},
   "source": [
    "### Combined Rollout: Full Surrogate Inference\n",
    "Given only an initial condition, predict both state trajectory AND derived quantities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43a2942f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load test data\n",
    "test_data = np.load(data_path)\n",
    "test_state = test_data['state']\n",
    "test_derived = test_data['derived']\n",
    "\n",
    "# Use last 10% as test\n",
    "test_start = int(len(test_state) * 0.9)\n",
    "test_state = test_state[test_start:]\n",
    "test_derived = test_derived[test_start:]\n",
    "\n",
    "print(f\"Test state shape: {test_state.shape}\")\n",
    "print(f\"Test derived shape: {test_derived.shape}\")\n",
    "\n",
    "# Run combined rollout using shared utility\n",
    "num_steps = min(len(test_state), 200)\n",
    "state_recon, derived_recon = rollout_combined(\n",
    "    state_model, derived_model, test_state[0], num_steps, device\n",
    ")\n",
    "\n",
    "print(f\"\\nState recon: {state_recon.shape}, Derived recon: {derived_recon.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fa3e31f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluation\n",
    "fig, axes = plt.subplots(2, 2, figsize=(12, 8))\n",
    "\n",
    "# Gamma_n comparison\n",
    "axes[0, 0].plot(test_derived[:num_steps, 0], 'b-', label='Ground Truth', alpha=0.8)\n",
    "axes[0, 0].plot(derived_recon[:, 0], 'r--', label='FNO Predicted', alpha=0.8)\n",
    "axes[0, 0].set_xlabel('Timestep'); axes[0, 0].set_ylabel('gamma_n')\n",
    "axes[0, 0].set_title('Gamma_n'); axes[0, 0].legend(); axes[0, 0].grid(True, alpha=0.3)\n",
    "\n",
    "# Gamma_c comparison\n",
    "axes[0, 1].plot(test_derived[:num_steps, 1], 'b-', label='Ground Truth', alpha=0.8)\n",
    "axes[0, 1].plot(derived_recon[:, 1], 'r--', label='FNO Predicted', alpha=0.8)\n",
    "axes[0, 1].set_xlabel('Timestep'); axes[0, 1].set_ylabel('gamma_c')\n",
    "axes[0, 1].set_title('Gamma_c'); axes[0, 1].legend(); axes[0, 1].grid(True, alpha=0.3)\n",
    "\n",
    "# Derived error over time\n",
    "gamma_n_err = np.abs(test_derived[:num_steps, 0] - derived_recon[:, 0])\n",
    "gamma_c_err = np.abs(test_derived[:num_steps, 1] - derived_recon[:, 1])\n",
    "axes[1, 0].plot(gamma_n_err, label='gamma_n')\n",
    "axes[1, 0].plot(gamma_c_err, label='gamma_c')\n",
    "axes[1, 0].set_xlabel('Timestep'); axes[1, 0].set_ylabel('Abs Error')\n",
    "axes[1, 0].set_title('Derived Quantity Error'); axes[1, 0].legend(); axes[1, 0].grid(True, alpha=0.3)\n",
    "\n",
    "# State MSE over time\n",
    "state_mse = compute_mse_over_time(test_state[:num_steps], state_recon)\n",
    "axes[1, 1].plot(state_mse)\n",
    "axes[1, 1].set_xlabel('Timestep'); axes[1, 1].set_ylabel('MSE')\n",
    "axes[1, 1].set_title('State Prediction Error'); axes[1, 1].set_yscale('log'); axes[1, 1].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Summary\n",
    "print(\"\\n\" + \"=\" * 50)\n",
    "print_summary(\"Gamma_n MAE\", gamma_n_err)\n",
    "print_summary(\"Gamma_c MAE\", gamma_c_err)\n",
    "print_summary(\"State MSE\", state_mse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ebc794c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Animation: State reconstruction (if animations enabled)\n",
    "if show_animations:\n",
    "    print(\"\\033[1mShowing state reconstruction animation\")\n",
    "    \n",
    "    # Subsample for animation\n",
    "    step = 5\n",
    "    gt_subbed = test_state[:num_steps:step]\n",
    "    recon_subbed = state_recon[::step]\n",
    "    \n",
    "    fig, axes = plt.subplots(1, 2, figsize=(10, 4))\n",
    "    \n",
    "    vmin = gt_subbed[:, 0].min()\n",
    "    vmax = gt_subbed[:, 0].max()\n",
    "    \n",
    "    img1 = axes[0].imshow(gt_subbed[0, 0], vmin=vmin, vmax=vmax, cmap='viridis')\n",
    "    img2 = axes[1].imshow(recon_subbed[0, 0], vmin=vmin, vmax=vmax, cmap='viridis')\n",
    "    \n",
    "    axes[0].set_title(\"Ground Truth (density)\")\n",
    "    axes[1].set_title(\"Surrogate Prediction\")\n",
    "    \n",
    "    plt.colorbar(img1, ax=axes[0], fraction=0.046)\n",
    "    plt.colorbar(img2, ax=axes[1], fraction=0.046)\n",
    "    \n",
    "    def animate(frame):\n",
    "        img1.set_data(gt_subbed[frame, 0])\n",
    "        img2.set_data(recon_subbed[frame, 0])\n",
    "        return [img1, img2]\n",
    "    \n",
    "    animation = anim.FuncAnimation(fig, animate, frames=len(gt_subbed), interval=50, blit=True)\n",
    "    display(HTML(animation.to_jshtml()))\n",
    "else:\n",
    "    print(\"Animations disabled\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (myenv)",
   "language": "python",
   "name": "myenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
